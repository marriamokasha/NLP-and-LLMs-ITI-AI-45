{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955533a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import cupy as cp\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import ast\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e833c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  spacy.require_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77ccb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset folder:\n",
      "['ner.csv']\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"naseralqaydeh/named-entity-recognition-ner-corpus\")\n",
    "print(\"Files in dataset folder:\")\n",
    "print(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6297f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                           Sentence  \\\n",
       "0  Sentence: 1  Thousands of demonstrators have marched throug...   \n",
       "1  Sentence: 2  Families of soldiers killed in the conflict jo...   \n",
       "2  Sentence: 3  They marched from the Houses of Parliament to ...   \n",
       "3  Sentence: 4  Police put the number of marchers at 10,000 wh...   \n",
       "4  Sentence: 5  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
       "1  ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...   \n",
       "2  ['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...   \n",
       "3  ['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...   \n",
       "4  ['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...  \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(path, \"ner.csv\"))  # replace with actual filename\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4622eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_row_to_spacy_format(sentence, tags_raw):\n",
    "    tags = ast.literal_eval(tags_raw)\n",
    "    words = sentence.split()\n",
    "\n",
    "    if len(words) != len(tags):\n",
    "        return None  # or raise warning/log\n",
    "\n",
    "    entities = []\n",
    "    start = 0\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        word = words[i]\n",
    "        tag = tags[i]\n",
    "        word_start = sentence.find(word, start)\n",
    "        word_end = word_start + len(word)\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            ent_type = tag[2:]\n",
    "            ent_start = word_start\n",
    "            ent_end = word_end\n",
    "            j = i + 1\n",
    "            while j < len(tags) and tags[j] == f\"I-{ent_type}\":\n",
    "                next_word = words[j]\n",
    "                next_start = sentence.find(next_word, ent_end)\n",
    "                ent_end = next_start + len(next_word)\n",
    "                j += 1\n",
    "            entities.append((ent_start, ent_end, ent_type))\n",
    "            i = j\n",
    "            start = ent_end\n",
    "        else:\n",
    "            i += 1\n",
    "            start = word_end\n",
    "\n",
    "    return (sentence, {\"entities\": entities})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022bbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA  = []\n",
    "for _, row in df.iterrows():\n",
    "    item = convert_sentence_row_to_spacy_format(row[\"Sentence\"], row[\"Tag\"])\n",
    "    if item:\n",
    "        TRAIN_DATA .append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4939b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(TRAIN_DATA, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71f3cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner = nlp.get_pipe(\"ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa446293",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations[\"entities\"]:\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df6c77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.resume_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f0da522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 36772.81610529137}\n",
      "Epoch 2: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 28590.879614144964}\n",
      "Epoch 3: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 25736.831421646577}\n",
      "Epoch 4: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 23576.559229979594}\n",
      "Epoch 5: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 22179.320753644923}\n",
      "Epoch 6: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 20820.874487552646}\n",
      "Epoch 7: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 19541.59834955089}\n",
      "Epoch 8: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 18526.303503396768}\n",
      "Epoch 9: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 17484.77621702419}\n",
      "Epoch 10: Loss = {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 16859.61960660386}\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 8\n",
    "for epoch in range(epochs):\n",
    "    losses = {}\n",
    "    random.shuffle(train_data)\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        batch = train_data[i:i + batch_size]\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in batch]\n",
    "        nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3d5ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation example outputs:\n",
      "\n",
      "Text: U.S. Secretary of State Condoleezza Rice , who is on a tour of the Middle East , met with Egyptian President Hosni Mubarak Wednesday .\n",
      "Predicted entities: [('U.S.', 'org'), ('State', 'org'), ('Condoleezza Rice', 'per'), ('Middle East', 'geo'), ('Egyptian', 'gpe'), ('President Hosni Mubarak', 'per'), ('Wednesday', 'tim')]\n",
      "True entities: [(0, 4, 'org'), (24, 40, 'per'), (67, 78, 'geo'), (90, 98, 'gpe'), (99, 122, 'per'), (123, 132, 'tim')]\n",
      "\n",
      "Text: President Maumoon Abdul GAYOOM dominated the islands ' political scene for 30 years , elected to six successive terms by single-party referendums .\n",
      "Predicted entities: [('President Maumoon Abdul GAYOOM', 'per'), ('30', 'tim')]\n",
      "True entities: [(0, 30, 'per'), (75, 77, 'tim')]\n",
      "\n",
      "Text: Ms. Rice noted that President Bush 's Emergency Plan for AIDS Relief is the largest international initiative ever undertaken by a single nation to combat a disease .\n",
      "Predicted entities: [('Ms. Rice', 'per'), ('President Bush', 'per'), ('AIDS Relief', 'org')]\n",
      "True entities: [(0, 8, 'per'), (20, 34, 'per'), (57, 68, 'org')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidation example outputs:\")\n",
    "for text, annotations in random.sample(val_data, 3):\n",
    "    doc = nlp(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Predicted entities:\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"True entities:\", annotations[\"entities\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_NLP_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
